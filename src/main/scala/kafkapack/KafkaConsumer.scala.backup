package kafkapack
import java.util.Properties
import contextpack._
import scala.collection.JavaConverters._
import org.apache.kafka.common.serialization.StringDeserializer 
import org.apache.spark.streaming.kafka010._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.TaskContext
import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.sql._
import org.apache.spark.streaming.dstream.InputDStream
import org.apache.spark.rdd.RDD

class ConsumerStreaming2(createRDD: () => RDD[String]) extends Serializable {

    val rdd = createRDD()
}

object ConsumerStreaming2 {
  //read from kafka 

  def apply(topic: String): RDD[String] = {
    
    val sconf = MainContext.getSparkConf()
    val sc = new SparkContext(sconf)
    val ssc = new StreamingContext(sc, Seconds(2))
    val ssess = SparkSession.builder.config(sc.getConf).getOrCreate()


    val kafkaParams = Map[String, Object](
    "bootstrap.servers" -> "localhost:9092",
    "key.deserializer" -> classOf[StringDeserializer],
    "value.deserializer" -> classOf[StringDeserializer],
    "group.id" -> "use_a_separate_group_id_for_each_stream",
    "auto.offset.reset" -> "latest",
    "enable.auto.commit" -> (false: java.lang.Boolean)
     )

    //topics has to be Array type, not Strings
    val topics = Array(topic)
    //val ssc = MainContext.getStreamingContext()
    val topicdstream = KafkaUtils.createDirectStream[String, String](
      // StreamingContext below, get current running StreamingContext imported from context package
      ssc,
      PreferConsistent,
      Subscribe[String, String](topics, kafkaParams)
    )

      
      val now = System.currentTimeMillis()
    println(s"(Consumer) Current unix time is: $now")

    //testing offsets
    // topicdstream.foreachRDD { rdd => 
    //   val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
    //   rdd.foreachPartition { iter =>
    //     val o: OffsetRange = offsetRanges(TaskContext.get.partitionId)
    //     println(s"${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}")
    //   }
    // }
    
    val dataset = sc.emptyRDD[String]
        //windowStream method
        // val windowStream = topicdstream.window(Minutes(1))
        // windowStream.transform{rdd => rdd.join(dataset)}
    topicdstream.foreachRDD { rdd=>rdd.join(dataset)
        }
        dataset
    



    ssc.start()             // Start the computation
    ssc.awaitTermination()  // Wait for the computation to terminate
    new ConsumerStreaming2(rddFunc)


  }

  
}
